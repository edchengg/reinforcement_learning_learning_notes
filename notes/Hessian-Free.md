Very good derivation

http://andrew.gibiansky.com/blog/machine-learning/hessian-free-optimization/

The paper
http://www.cs.toronto.edu/~jmartens/docs/Deep_HessianFree.pdf

Hessian Free is basically an extention of Newton's second order method, but eliminates the need to invert the entire Hessian Matrix.

1. Gradient Descent
Gradient Descent: fixed step size means dx = \epsilon \nabla y. When \nabla y is small, dx is small. However, dx should be long because 
1. Newton Methods
1. Conjugate Gradient
1. Hassian Free
